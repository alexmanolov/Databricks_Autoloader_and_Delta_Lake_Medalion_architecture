{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98add118-6c4f-4e44-93a4-3e3196b73223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Task 3\n",
    "\n",
    "Overview: Creation of gold layer. Create standard star schema based on the bellow\n",
    "specifications\n",
    "- Consider only type 2-dimension fields when calculating history for the dim.\n",
    "- Each row should have effective start and effective end dates, which will\n",
    "represent the time row was active. Base them on silver layer\n",
    "consume_datetime ;\n",
    "- Make sure the referential integrity between the fact and dimensions is kept\n",
    "- No duplicates should be kept, and history should be only for specified SCD2\n",
    "fields.\n",
    "- Use xxhash64 function for hashing, where applicable.\n",
    "- Dimension details:\n",
    "dim_customersField name Description Dimension Type\n",
    "cust_sk surrgogate_key - hashed\n",
    "cust_nk\n",
    "cust_nk natural key (customer_id)\n",
    "cust_first_name 1\n",
    "cust_last_name 1\n",
    "cust_address_country_id 2\n",
    "cust_address_state_province 2\n",
    "cust_address_city 2\n",
    "cust_address_postal_code 2\n",
    "cust_address_street_address 2\n",
    "cust_phone_number 1\n",
    "cust_email 1\n",
    "account_mgr_id 1\n",
    "date_of_birth 1\n",
    "marital_status 2\n",
    "Gender 1\n",
    "effective_from effective from date for type\n",
    "2 dimensions based on the\n",
    "consume_datetime in silver\n",
    "layer\n",
    "effective_to effective to date for type 2\n",
    "dimensions based on the\n",
    "consume_datetime in silver\n",
    "layer\n",
    "Inserted_datetime when row was inserted\n",
    "Updated_datetime when row was last updated\n",
    "dim_products\n",
    "Field name Description Dimension Type\n",
    "product_sk surrgogate_key - hashed\n",
    "product_id\n",
    "product_nk natural key (product_id)\n",
    "product_name 1\n",
    "category_name 2\n",
    "weight_class 1\n",
    "product_status 2\n",
    "list_price 2\n",
    "min_price 1\n",
    "effective_from effective from date for type\n",
    "2 dimensions based on the\n",
    "consume_timestamp in\n",
    "silver layereffective_to effective to date for type 2\n",
    "dimensions based on the\n",
    "consume_timestamp in\n",
    "silver layer\n",
    "Inserted_datetime when row was inserted\n",
    "Updated_datetime when row was last updated\n",
    "Fact orders\n",
    "Field name Description\n",
    "order_sk surrgogate_key - hashed order_nk\n",
    "customer_sk dimension surrgogate key\n",
    "product_sk dimension surrgogate key\n",
    "order_nk natural key - Concatenation of\n",
    "order_id,line_item_id,customer_id,product_id\n",
    "split by pipes\n",
    "customer_nk dimension natural key\n",
    "product_nk dimension natural key\n",
    "order_id\n",
    "line_item_id\n",
    "order_date\n",
    "order_mode\n",
    "order_status\n",
    "unit_price\n",
    "quantity\n",
    "Inserted_datetime when row was inserted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6060a56-444a-4939-8639-a68f10bd1119",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#################\n",
    "##### Gold ######\n",
    "#################\n",
    "\n",
    "\n",
    "###################\n",
    "## dim_customers ##\n",
    "###################\n",
    "\n",
    "from pyspark.sql.functions import col, lit, xxhash64, current_timestamp\n",
    "\n",
    "# Load silver customers\n",
    "silver_customers_df = spark.table(\"de_pyspark_training_catalog.buddy_group_1.amanolov_silver_customers_exam\")\n",
    "\n",
    "# Build staging DataFrame\n",
    "staging_customers_df = silver_customers_df.select(\n",
    "    col(\"customer_id\").alias(\"cust_nk\"),\n",
    "    col(\"cust_first_name\"),\n",
    "    col(\"cust_last_name\"),\n",
    "    col(\"cust_address_country_id\"),\n",
    "    col(\"cust_address_state_province\"),\n",
    "    col(\"cust_address_city\"),\n",
    "    col(\"cust_address_postal_code\"),\n",
    "    col(\"cust_address_street_address\"),\n",
    "    col(\"phone_number\").alias(\"cust_phone_number\"),\n",
    "    col(\"cust_email\"),\n",
    "    \"account_mgr_id\",\n",
    "    \"date_of_birth\",\n",
    "    \"marital_status\",\n",
    "    \"gender\",\n",
    "    \"consume_datetime\"\n",
    ")\n",
    "\n",
    "# Add surrogate key\n",
    "staging_customers_df = staging_customers_df.withColumn(\"cust_sk\", xxhash64(\"cust_nk\"))\n",
    "\n",
    "# Add control columns\n",
    "staging_customers_df = staging_customers_df.withColumn(\"inserted_datetime\", current_timestamp())\n",
    "staging_customers_df = staging_customers_df.withColumn(\"updated_datetime\", current_timestamp())\n",
    "staging_customers_df = staging_customers_df.withColumn(\"effective_from\", col(\"consume_datetime\"))\n",
    "staging_customers_df = staging_customers_df.withColumn(\"effective_to\", lit(\"9999-12-31\").cast(\"date\"))\n",
    "\n",
    "# Save staging DataFrame as a temporary view\n",
    "staging_customers_df.createOrReplaceTempView(\"staging_customers\")\n",
    "\n",
    "# Create gold dimension table if not exists \n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS de_pyspark_training_catalog.buddy_group_1.amanolov_gold_dim_customers_exam (\n",
    "        cust_sk BIGINT,\n",
    "        cust_nk STRING,\n",
    "        cust_first_name STRING,\n",
    "        cust_last_name STRING,\n",
    "        cust_address_country_id STRING,\n",
    "        cust_address_state_province STRING,\n",
    "        cust_address_city STRING,\n",
    "        cust_address_postal_code STRING,\n",
    "        cust_address_street_address STRING,\n",
    "        cust_phone_number STRING,\n",
    "        cust_email STRING,\n",
    "        account_mgr_id STRING,\n",
    "        date_of_birth DATE,\n",
    "        marital_status STRING,\n",
    "        gender STRING,\n",
    "        effective_from TIMESTAMP,\n",
    "        effective_to DATE,\n",
    "        inserted_datetime TIMESTAMP,\n",
    "        updated_datetime TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Apply SCD Type 2 Merge\n",
    "spark.sql(\"\"\"\n",
    "    MERGE INTO de_pyspark_training_catalog.buddy_group_1.amanolov_gold_dim_customers_exam AS target\n",
    "    USING staging_customers AS source\n",
    "    ON target.cust_nk = source.cust_nk AND target.effective_to = '9999-12-31'\n",
    "\n",
    "    WHEN MATCHED AND (\n",
    "        target.cust_address_country_id <> source.cust_address_country_id OR\n",
    "        target.cust_address_state_province <> source.cust_address_state_province OR\n",
    "        target.cust_address_city <> source.cust_address_city OR\n",
    "        target.cust_address_postal_code <> source.cust_address_postal_code OR\n",
    "        target.cust_address_street_address <> source.cust_address_street_address OR\n",
    "        target.marital_status <> source.marital_status\n",
    "    ) THEN\n",
    "      -- Close the existing record\n",
    "      UPDATE SET\n",
    "        target.effective_to = source.consume_datetime,\n",
    "        target.updated_datetime = current_timestamp()\n",
    "\n",
    "    WHEN MATCHED THEN\n",
    "      -- SCD Type 1 fields update in place\n",
    "      UPDATE SET\n",
    "        target.cust_first_name = source.cust_first_name,\n",
    "        target.cust_last_name = source.cust_last_name,\n",
    "        target.cust_phone_number = source.cust_phone_number,\n",
    "        target.cust_email = source.cust_email,\n",
    "        target.account_mgr_id = source.account_mgr_id,\n",
    "        target.date_of_birth = source.date_of_birth,\n",
    "        target.gender = source.gender,\n",
    "        target.updated_datetime = current_timestamp()\n",
    "\n",
    "    WHEN NOT MATCHED THEN\n",
    "      -- Insert new record\n",
    "      INSERT (\n",
    "        cust_sk,\n",
    "        cust_nk,\n",
    "        cust_first_name,\n",
    "        cust_last_name,\n",
    "        cust_address_country_id,\n",
    "        cust_address_state_province,\n",
    "        cust_address_city,\n",
    "        cust_address_postal_code,\n",
    "        cust_address_street_address,\n",
    "        cust_phone_number,\n",
    "        cust_email,\n",
    "        account_mgr_id,\n",
    "        date_of_birth,\n",
    "        marital_status,\n",
    "        gender,\n",
    "        effective_from,\n",
    "        effective_to,\n",
    "        inserted_datetime,\n",
    "        updated_datetime\n",
    "      )\n",
    "      VALUES (\n",
    "        source.cust_sk,\n",
    "        source.cust_nk,\n",
    "        source.cust_first_name,\n",
    "        source.cust_last_name,\n",
    "        source.cust_address_country_id,\n",
    "        source.cust_address_state_province,\n",
    "        source.cust_address_city,\n",
    "        source.cust_address_postal_code,\n",
    "        source.cust_address_street_address,\n",
    "        source.cust_phone_number,\n",
    "        source.cust_email,\n",
    "        source.account_mgr_id,\n",
    "        source.date_of_birth,\n",
    "        source.marital_status,\n",
    "        source.gender,\n",
    "        source.effective_from,\n",
    "        source.effective_to,\n",
    "        current_timestamp(),\n",
    "        current_timestamp()\n",
    "      )\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09563051-66a4-4e93-9199-a9ad2c8bc5c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#################\n",
    "##### Gold ######\n",
    "#################\n",
    "\n",
    "###################\n",
    "## dim_products ##\n",
    "###################\n",
    "\n",
    "from pyspark.sql.functions import col, lit, xxhash64, current_timestamp\n",
    "\n",
    "# Load silver products\n",
    "silver_products_df = spark.table(\"de_pyspark_training_catalog.buddy_group_1.amanolov_silver_products_exam\")\n",
    "\n",
    "# Build gold dim_products (staging DataFrame)\n",
    "staging_products_df = silver_products_df.select(\n",
    "    col(\"product_id\").alias(\"product_nk\"),\n",
    "    col(\"product_name\"),\n",
    "    col(\"category_name\"),\n",
    "    col(\"weight_class\"),\n",
    "    col(\"product_status\"),\n",
    "    col(\"list_price\"),\n",
    "    col(\"min_price\"),\n",
    "    \"consume_datetime\"\n",
    ")\n",
    "\n",
    "# Add surrogate key\n",
    "staging_products_df = staging_products_df.withColumn(\"product_sk\", xxhash64(\"product_nk\"))\n",
    "\n",
    "# Add control columns\n",
    "staging_products_df = staging_products_df.withColumn(\"inserted_datetime\", current_timestamp())\n",
    "staging_products_df = staging_products_df.withColumn(\"updated_datetime\", current_timestamp())\n",
    "staging_products_df = staging_products_df.withColumn(\"effective_from\", col(\"consume_datetime\"))\n",
    "staging_products_df = staging_products_df.withColumn(\"effective_to\", lit(\"9999-12-31\").cast(\"date\"))\n",
    "\n",
    "# Save staging DataFrame as a temporary view\n",
    "staging_products_df.createOrReplaceTempView(\"staging_products\")\n",
    "\n",
    "# Create gold dimension table if not exists\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS de_pyspark_training_catalog.buddy_group_1.amanolov_gold_dim_products_exam (\n",
    "        product_sk BIGINT,\n",
    "        product_nk STRING,\n",
    "        product_name STRING,\n",
    "        category_name STRING,\n",
    "        weight_class STRING,\n",
    "        product_status STRING,\n",
    "        list_price DOUBLE,\n",
    "        min_price DOUBLE,\n",
    "        consume_datetime TIMESTAMP,\n",
    "        effective_from TIMESTAMP,\n",
    "        effective_to DATE,\n",
    "        inserted_datetime TIMESTAMP,\n",
    "        updated_datetime TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Apply SCD Type 2 Merge\n",
    "spark.sql(\"\"\"\n",
    "    MERGE INTO de_pyspark_training_catalog.buddy_group_1.amanolov_gold_dim_products_exam AS target\n",
    "    USING staging_products AS source\n",
    "    ON target.product_nk = source.product_nk AND target.effective_to = '9999-12-31'\n",
    "\n",
    "    WHEN MATCHED AND (\n",
    "        target.category_name <> source.category_name OR\n",
    "        target.product_status <> source.product_status OR\n",
    "        target.list_price <> source.list_price\n",
    "    ) THEN\n",
    "      -- Close the existing record\n",
    "      UPDATE SET\n",
    "        target.effective_to = source.consume_datetime,\n",
    "        target.updated_datetime = current_timestamp()\n",
    "\n",
    "    WHEN MATCHED THEN\n",
    "      -- SCD Type 1 fields update in place\n",
    "      UPDATE SET\n",
    "        target.product_name = source.product_name,\n",
    "        target.weight_class = source.weight_class,\n",
    "        target.min_price = source.min_price,\n",
    "        target.updated_datetime = current_timestamp()\n",
    "\n",
    "    WHEN NOT MATCHED THEN\n",
    "      -- Insert new record\n",
    "      INSERT (\n",
    "        product_sk,\n",
    "        product_nk,\n",
    "        product_name,\n",
    "        category_name,\n",
    "        weight_class,\n",
    "        product_status,\n",
    "        list_price,\n",
    "        min_price,\n",
    "        consume_datetime,\n",
    "        effective_from,\n",
    "        effective_to,\n",
    "        inserted_datetime,\n",
    "        updated_datetime\n",
    "      )\n",
    "      VALUES (\n",
    "        source.product_sk,\n",
    "        source.product_nk,\n",
    "        source.product_name,\n",
    "        source.category_name,\n",
    "        source.weight_class,\n",
    "        source.product_status,\n",
    "        source.list_price,\n",
    "        source.min_price,\n",
    "        source.consume_datetime,\n",
    "        source.effective_from,\n",
    "        source.effective_to,\n",
    "        current_timestamp(),\n",
    "        current_timestamp()\n",
    "      )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a791bbdc-5dc5-48fe-b52c-2eee5f2d9340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#################\n",
    "##### Gold ######\n",
    "#################\n",
    "\n",
    "###################\n",
    "### fact_orders ###\n",
    "###################\n",
    "\n",
    "from pyspark.sql.functions import col, lit, xxhash64, current_timestamp, concat_ws\n",
    "\n",
    "# Load silver order_items table containing line_item_id, product_id\n",
    "silver_order_items_df = spark.table(\"de_pyspark_training_catalog.buddy_group_1.amanolov_silver_order_items_exam\")\n",
    "\n",
    "# Load silver orders table containing customer_id, order details\n",
    "silver_orders_df = spark.table(\"de_pyspark_training_catalog.buddy_group_1.amanolov_silver_orders_exam\")\n",
    "\n",
    "# Load gold dims\n",
    "dim_customers_df = spark.table(\"de_pyspark_training_catalog.buddy_group_1.amanolov_gold_dim_customers_exam\")\n",
    "dim_products_df = spark.table(\"de_pyspark_training_catalog.buddy_group_1.amanolov_gold_dim_products_exam\")\n",
    "\n",
    "# Join order_items with orders to get customer_id and order details\n",
    "fact_orders_df = silver_order_items_df.join(\n",
    "    silver_orders_df.drop(\"inserted_datetime\"),  # Drop to avoid column name duplication\n",
    "    on=\"order_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Build order_nk (natural key)\n",
    "fact_orders_df = fact_orders_df.withColumn(\n",
    "    \"order_nk\",\n",
    "    concat_ws(\"|\",\n",
    "              col(\"order_id\").cast(\"string\"),\n",
    "              col(\"line_item_id\").cast(\"string\"),\n",
    "              col(\"customer_id\").cast(\"string\"),\n",
    "              col(\"product_id\").cast(\"string\"))\n",
    ")\n",
    "\n",
    "# Build surrogate key\n",
    "fact_orders_df = fact_orders_df.withColumn(\"order_sk\", xxhash64(\"order_nk\"))\n",
    "\n",
    "# SCD2 Join to dim_customers (Time-aware)\n",
    "fact_orders_df = fact_orders_df.join(\n",
    "    dim_customers_df.select(\"cust_sk\", \"cust_nk\", \"effective_from\", \"effective_to\"),\n",
    "    (fact_orders_df.customer_id == dim_customers_df.cust_nk) &\n",
    "    (fact_orders_df.order_date >= dim_customers_df.effective_from) &\n",
    "    (fact_orders_df.order_date < dim_customers_df.effective_to),\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# SCD2 Join to dim_products (Time-aware)\n",
    "fact_orders_df = fact_orders_df.join(\n",
    "    dim_products_df.select(\"product_sk\", \"product_nk\", \"effective_from\", \"effective_to\"),\n",
    "    (fact_orders_df.product_id == dim_products_df.product_nk) &\n",
    "    (fact_orders_df.order_date >= dim_products_df.effective_from) &\n",
    "    (fact_orders_df.order_date < dim_products_df.effective_to),\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Add inserted_datetime\n",
    "fact_orders_df = fact_orders_df.withColumn(\"inserted_datetime\", current_timestamp())\n",
    "\n",
    "# Select final columns per the PDF\n",
    "fact_orders_df = fact_orders_df.select(\n",
    "    \"order_sk\",\n",
    "    \"cust_sk\",\n",
    "    \"product_sk\",\n",
    "    \"order_nk\",\n",
    "    col(\"customer_id\").alias(\"customer_nk\"),\n",
    "    col(\"product_id\").alias(\"product_nk\"),\n",
    "    \"order_id\",\n",
    "    \"line_item_id\",\n",
    "    \"order_date\",\n",
    "    \"order_mode\",\n",
    "    \"order_status\",\n",
    "    \"unit_price\",\n",
    "    \"quantity\",\n",
    "    \"inserted_datetime\"\n",
    ")\n",
    "\n",
    "# Save as gold fact table\n",
    "fact_orders_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"de_pyspark_training_catalog.buddy_group_1.amanolov_gold_fact_orders_exam\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5375633204748151,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Final exam",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
