# Databricks Autoloader and Delta Lake Medallion Architecture

## ğŸ“‚ Project Overview

This project showcases the development of a complete **OLTP to OLAP data pipeline** using Databricks, PySpark, and Delta Lake, following the **Medallion Architecture** (Bronze, Silver, Gold layers).

The pipeline processes source data related to customers, orders, order items, and products, and transforms it into clean, deduplicated, and analytics-ready datasets.

## ğŸ’¡ Key Features

- âœ… Real-time ingestion with **Databricks Autoloader**
- âœ… Data storage in **Delta Lake format** with ACID compliance
- âœ… Control columns: `ingest_datetime` and `batch_id` for auditability
- âœ… Multi-layer data processing: **Bronze, Silver, Gold tables**
- âœ… Deduplication using PySpark Window functions
- âœ… Partitioning and schema evolution for scalability

## ğŸ“„ Dataset Overview

The project ingests and processes the following datasets:

- **Customers**
- **Orders**
- **Order Items**
- **Products**

The datasets are loaded from simulated S3 paths and follow CSV format.

## ğŸ”§ Technologies Used

- Databricks
- PySpark
- Delta Lake
- Databricks Autoloader
- AWS S3 (simulated)

## ğŸ“… Project Structure

```
Databricks_Autoloader_and_Delta_Lake_Medalion_architecture/
â”œâ”€â”€ bronze_layer_notebook.ipynb
â”œâ”€â”€ silver_layer_notebook.ipynb
â”œâ”€â”€ gold_layer_notebook.ipynb
â”œâ”€â”€ README.md
â””â”€â”€ resources/
    â””â”€â”€ sample_data (simulated)
```

## ğŸš€ How to Run

1. Import the notebooks into your Databricks workspace.
2. Adjust the S3 paths or mount points to your storage.
3. Run the notebooks in sequence:
   - Bronze Layer (Autoloader ingestion)
   - Silver Layer (Cleansing and deduplication)
   - Gold Layer (Analytics-ready output)
4. Monitor Autoloader streaming progress via Databricks UI.

## ğŸ“Š Learning Outcomes

- Building robust pipelines with Databricks Autoloader
- Applying the Medallion Architecture effectively
- Using Delta Lake for efficient data storage and time travel
- Performing data transformations and deduplication with PySpark

## ğŸ“ Generating Test Data

If you would like to generate sample data to test the pipeline:

1. Install the required Python library:

```python
%pip install faker
```

2. Use the provided **test data generation notebook** to create synthetic addresses, cities, states, and postal codes for the customer dataset.

3. Example functions include:

```python
from faker import Faker
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

faker = Faker()

@udf(StringType())
def fake_country():
    return faker.country_code(representation="alpha-2")

@udf(StringType())
def fake_city():
    return faker.city()
```

4. The generated data can then be injected into the Autoloader pipeline as test input.

This approach allows you to simulate data ingestion and verify pipeline robustness in a controlled environment.

## ğŸ’ Author

**Aleksandar Manolov**\
*Data Engineer in training | Aspiring Data Architect*

---

If you found this project useful or interesting, feel free to star the repository or reach out for collaboration!

---

*Project: Databricks\_Autoloader\_and\_Delta\_Lake\_Medalion\_architecture*

