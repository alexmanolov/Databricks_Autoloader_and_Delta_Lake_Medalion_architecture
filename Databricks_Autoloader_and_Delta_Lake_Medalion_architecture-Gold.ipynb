{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98add118-6c4f-4e44-93a4-3e3196b73223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Task 3\n",
    "\n",
    "Overview: Creation of gold layer. Create standard star schema based on the bellow\n",
    "specifications\n",
    "- Consider only type 2-dimension fields when calculating history for the dim.\n",
    "- Each row should have effective start and effective end dates, which will\n",
    "represent the time row was active. Base them on silver layer\n",
    "consume_datetime ;\n",
    "- Make sure the referential integrity between the fact and dimensions is kept\n",
    "- No duplicates should be kept, and history should be only for specified SCD2\n",
    "fields.\n",
    "- Use xxhash64 function for hashing, where applicable.\n",
    "- Dimension details:\n",
    "dim_customersField name Description Dimension Type\n",
    "cust_sk surrgogate_key - hashed\n",
    "cust_nk\n",
    "cust_nk natural key (customer_id)\n",
    "cust_first_name 1\n",
    "cust_last_name 1\n",
    "cust_address_country_id 2\n",
    "cust_address_state_province 2\n",
    "cust_address_city 2\n",
    "cust_address_postal_code 2\n",
    "cust_address_street_address 2\n",
    "cust_phone_number 1\n",
    "cust_email 1\n",
    "account_mgr_id 1\n",
    "date_of_birth 1\n",
    "marital_status 2\n",
    "Gender 1\n",
    "effective_from effective from date for type\n",
    "2 dimensions based on the\n",
    "consume_datetime in silver\n",
    "layer\n",
    "effective_to effective to date for type 2\n",
    "dimensions based on the\n",
    "consume_datetime in silver\n",
    "layer\n",
    "Inserted_datetime when row was inserted\n",
    "Updated_datetime when row was last updated\n",
    "dim_products\n",
    "Field name Description Dimension Type\n",
    "product_sk surrgogate_key - hashed\n",
    "product_id\n",
    "product_nk natural key (product_id)\n",
    "product_name 1\n",
    "category_name 2\n",
    "weight_class 1\n",
    "product_status 2\n",
    "list_price 2\n",
    "min_price 1\n",
    "effective_from effective from date for type\n",
    "2 dimensions based on the\n",
    "consume_timestamp in\n",
    "silver layereffective_to effective to date for type 2\n",
    "dimensions based on the\n",
    "consume_timestamp in\n",
    "silver layer\n",
    "Inserted_datetime when row was inserted\n",
    "Updated_datetime when row was last updated\n",
    "Fact orders\n",
    "Field name Description\n",
    "order_sk surrgogate_key - hashed order_nk\n",
    "customer_sk dimension surrgogate key\n",
    "product_sk dimension surrgogate key\n",
    "order_nk natural key - Concatenation of\n",
    "order_id,line_item_id,customer_id,product_id\n",
    "split by pipes\n",
    "customer_nk dimension natural key\n",
    "product_nk dimension natural key\n",
    "order_id\n",
    "line_item_id\n",
    "order_date\n",
    "order_mode\n",
    "order_status\n",
    "unit_price\n",
    "quantity\n",
    "Inserted_datetime when row was inserted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6060a56-444a-4939-8639-a68f10bd1119",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#################\n",
    "##### Gold ######\n",
    "#################\n",
    "\n",
    "\n",
    "###################\n",
    "## dim_customers ##\n",
    "###################\n",
    "\n",
    "from pyspark.sql.functions import col, lit, xxhash64, current_timestamp\n",
    "\n",
    "# Load silver customers\n",
    "silver_customers_df = spark.table(\"de_pyspark_training_catalog.buddy_group_1.amanolov_silver_customers_exam\")\n",
    "\n",
    "# Build the gold dim_customers \n",
    "dim_customers_df = silver_customers_df.select(\n",
    "    col(\"customer_id\").alias(\"cust_nk\"),\n",
    "    col(\"cust_first_name\"),\n",
    "    col(\"cust_last_name\"),\n",
    "    col(\"cust_address_country_id\"),\n",
    "    col(\"cust_address_state_province\"),\n",
    "    col(\"cust_address_city\"),\n",
    "    col(\"cust_address_postal_code\"),\n",
    "    col(\"cust_address_street_address\"),\n",
    "    col(\"phone_number\").alias(\"cust_phone_number\"),\n",
    "    col(\"cust_email\"),\n",
    "    \"account_mgr_id\",\n",
    "    \"date_of_birth\",\n",
    "    \"marital_status\",\n",
    "    \"gender\",\n",
    "    \"consume_datetime\"\n",
    ")\n",
    "\n",
    "# Add surrogate key\n",
    "dim_customers_df = dim_customers_df.withColumn(\"cust_sk\", xxhash64(\"cust_nk\"))\n",
    "\n",
    "# Add timestamps\n",
    "dim_customers_df = dim_customers_df.withColumn(\"inserted_datetime\", current_timestamp())\n",
    "dim_customers_df = dim_customers_df.withColumn(\"updated_datetime\", current_timestamp())\n",
    "\n",
    "# Add SCD2 fields\n",
    "dim_customers_df = dim_customers_df.withColumn(\"effective_from\", col(\"consume_datetime\"))\n",
    "dim_customers_df = dim_customers_df.withColumn(\"effective_to\", lit(\"9999-12-31\").cast(\"date\"))\n",
    "\n",
    "# Drop consume_datetime if you donâ€™t want it in the final table\n",
    "dim_customers_df = dim_customers_df.drop(\"consume_datetime\")\n",
    "\n",
    "# Final column order\n",
    "dim_customers_df = dim_customers_df.select(\n",
    "    \"cust_sk\",\n",
    "    \"cust_nk\",\n",
    "    \"cust_first_name\",\n",
    "    \"cust_last_name\",\n",
    "    \"cust_address_country_id\",\n",
    "    \"cust_address_state_province\",\n",
    "    \"cust_address_city\",\n",
    "    \"cust_address_postal_code\",\n",
    "    \"cust_address_street_address\",\n",
    "    \"cust_phone_number\",\n",
    "    \"cust_email\",\n",
    "    \"account_mgr_id\",\n",
    "    \"date_of_birth\",\n",
    "    \"marital_status\",\n",
    "    \"gender\",\n",
    "    \"effective_from\",\n",
    "    \"effective_to\",\n",
    "    \"inserted_datetime\",\n",
    "    \"updated_datetime\"\n",
    ")\n",
    "\n",
    "# Save as gold table\n",
    "dim_customers_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"de_pyspark_training_catalog.buddy_group_1.amanolov_gold_dim_customers_exam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09563051-66a4-4e93-9199-a9ad2c8bc5c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#################\n",
    "##### Gold ######\n",
    "#################\n",
    "\n",
    "###################\n",
    "## dim_products ##\n",
    "###################\n",
    "\n",
    "from pyspark.sql.functions import col, lit, xxhash64, current_timestamp\n",
    "\n",
    "# Load silver products\n",
    "silver_products_df = spark.table(\"de_pyspark_training_catalog.buddy_group_1.amanolov_silver_products_exam\")\n",
    "\n",
    "# Build gold dim_products\n",
    "dim_products_df = silver_products_df.select(\n",
    "    col(\"product_id\").alias(\"product_nk\"),\n",
    "    col(\"product_name\"),\n",
    "    col(\"category_name\"),\n",
    "    col(\"weight_class\"),\n",
    "    col(\"product_status\"),\n",
    "    col(\"list_price\"),\n",
    "    col(\"min_price\"),\n",
    "    \"consume_datetime\"\n",
    ")\n",
    "\n",
    "# Add surrogate key\n",
    "dim_products_df = dim_products_df.withColumn(\"product_sk\", xxhash64(\"product_nk\"))\n",
    "\n",
    "# Add timestamps\n",
    "dim_products_df = dim_products_df.withColumn(\"inserted_datetime\", current_timestamp())\n",
    "dim_products_df = dim_products_df.withColumn(\"updated_datetime\", current_timestamp())\n",
    "\n",
    "# Add SCD2 fields\n",
    "dim_products_df = dim_products_df.withColumn(\"effective_from\", col(\"consume_datetime\"))\n",
    "dim_products_df = dim_products_df.withColumn(\"effective_to\", lit(\"9999-12-31\").cast(\"date\"))\n",
    "\n",
    "# Final column order\n",
    "dim_products_df = dim_products_df.select(\n",
    "    \"product_sk\",\n",
    "    \"product_nk\",\n",
    "    \"product_name\",\n",
    "    \"category_name\",\n",
    "    \"weight_class\",\n",
    "    \"product_status\",\n",
    "    \"list_price\",\n",
    "    \"min_price\",\n",
    "    \"consume_datetime\",\n",
    "    \"effective_from\",\n",
    "    \"effective_to\",\n",
    "    \"inserted_datetime\",\n",
    "    \"updated_datetime\"\n",
    ")\n",
    "\n",
    "# Save as gold table\n",
    "dim_products_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"de_pyspark_training_catalog.buddy_group_1.amanolov_gold_dim_products_exam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a791bbdc-5dc5-48fe-b52c-2eee5f2d9340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#################\n",
    "##### Gold ######\n",
    "#################\n",
    "\n",
    "###################\n",
    "### fact_orders ###\n",
    "###################\n",
    "\n",
    "from pyspark.sql.functions import col, lit, xxhash64, current_timestamp, concat_ws\n",
    "\n",
    "# Load silver order_items table containing line_item_id, product_id\n",
    "silver_order_items_df = spark.table(\"de_pyspark_training_catalog.buddy_group_1.amanolov_silver_order_items_exam\")\n",
    "\n",
    "# Load silver orders table containing customer_id, order details\n",
    "silver_orders_df = spark.table(\"de_pyspark_training_catalog.buddy_group_1.amanolov_silver_orders_exam\")\n",
    "\n",
    "# Load gold dims\n",
    "dim_customers_df = spark.table(\"de_pyspark_training_catalog.buddy_group_1.amanolov_gold_dim_customers_exam\")\n",
    "dim_products_df = spark.table(\"de_pyspark_training_catalog.buddy_group_1.amanolov_gold_dim_products_exam\")\n",
    "\n",
    "# Join order_items with orders to get customer_id and order details\n",
    "fact_orders_df = silver_order_items_df.join(\n",
    "    silver_orders_df.drop(\"inserted_datetime\"),  # Drop to avoid ambiguity, fresh timestamp will be added\n",
    "    on=\"order_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Build order_nk (natural key)\n",
    "fact_orders_df = fact_orders_df.withColumn(\n",
    "    \"order_nk\",\n",
    "    concat_ws(\"|\",\n",
    "              col(\"order_id\").cast(\"string\"),\n",
    "              col(\"line_item_id\").cast(\"string\"),\n",
    "              col(\"customer_id\").cast(\"string\"),\n",
    "              col(\"product_id\").cast(\"string\"))\n",
    ")\n",
    "\n",
    "# Build surrogate key\n",
    "fact_orders_df = fact_orders_df.withColumn(\"order_sk\", xxhash64(\"order_nk\"))\n",
    "\n",
    "# Join to dim_customers on natural key\n",
    "fact_orders_df = fact_orders_df.join(\n",
    "    dim_customers_df.select(\"cust_sk\", \"cust_nk\"),\n",
    "    fact_orders_df.customer_id == dim_customers_df.cust_nk,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Join to dim_products on natural key\n",
    "fact_orders_df = fact_orders_df.join(\n",
    "    dim_products_df.select(\"product_sk\", \"product_nk\"),\n",
    "    fact_orders_df.product_id == dim_products_df.product_nk,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Add inserted_datetime\n",
    "fact_orders_df = fact_orders_df.withColumn(\"inserted_datetime\", current_timestamp())\n",
    "\n",
    "# Select final columns exactly as per the PDF\n",
    "fact_orders_df = fact_orders_df.select(\n",
    "    \"order_sk\",\n",
    "    \"cust_sk\",\n",
    "    \"product_sk\",\n",
    "    \"order_nk\",\n",
    "    col(\"customer_id\").alias(\"customer_nk\"),\n",
    "    col(\"product_id\").alias(\"product_nk\"),\n",
    "    \"order_id\",\n",
    "    \"line_item_id\",\n",
    "    \"order_date\",\n",
    "    \"order_mode\",\n",
    "    \"order_status\",\n",
    "    \"unit_price\",\n",
    "    \"quantity\",\n",
    "    \"inserted_datetime\"\n",
    ")\n",
    "\n",
    "# Save as gold fact table\n",
    "fact_orders_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"de_pyspark_training_catalog.buddy_group_1.amanolov_gold_fact_orders_exam\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5375633204748151,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Final exam",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
