{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "836ad3f8-d26f-4958-8043-ca277b033476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install faker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a4b223d-076e-4487-a2f7-05d4812374d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "import random \n",
    "\n",
    "faker = Faker()\n",
    "Faker.seed(42) \n",
    " \n",
    "@F.udf(StringType())\n",
    "def fake_country(): return faker.country_code(representation=\"alpha-2\")\n",
    "\n",
    "@F.udf(StringType())\n",
    "def fake_state(): return faker.state_abbr()\n",
    "\n",
    "@F.udf(StringType())\n",
    "def fake_city(): return faker.city()\n",
    "\n",
    "@F.udf(StringType())\n",
    "def fake_postal(): return faker.postcode()\n",
    "\n",
    "@F.udf(StringType())\n",
    "def fake_street(): return faker.street_address() \n",
    "def sample_n_rows(df: DataFrame, n: int, seed: int = None) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame containing n random rows.\n",
    "    \n",
    "    :param df: Source Spark DataFrame\n",
    "    :param n: Number of random rows to sample\n",
    "    :param seed: Optional random seed for reproducibility\n",
    "    :return: Sampled DataFrame\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        return df.select(\"*\", F.rand(seed).alias('rand')).orderBy('rand').limit(n).drop('rand')\n",
    "    else:\n",
    "        return df.select(\"*\", F.rand().alias('rand')).orderBy('rand').limit(n).drop('rand')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a1847a0-7e48-4cad-bb12-fb82ed257a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Generate test data for customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f392d00-7ac2-4ff3-86ac-57ee0a5e4ff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"s3://de-40-training-raw/final_exam_data/customers/customers.csv\", header=True) \n",
    "df.schema \n",
    "# Assume df is your existing DataFrame\n",
    "sampled_df = sample_n_rows(df, 10)  # Take 10 random rows\n",
    "\n",
    "sampled_df.display()\n",
    "mocked_df = sampled_df.select('CUSTOMER_ID', 'CUST_FIRST_NAME', 'CUST_LAST_NAME', F.lit('US').alias('CUST_ADDRESS.COUNTRY_ID'), fake_state().alias('CUST_ADDRESS.STATE_PROVINCE'), fake_city().alias('CUST_ADDRESS.CITY'), fake_postal().alias('CUST_ADDRESS.POSTAL_CODE'), fake_street().alias('CUST_ADDRESS.STREET_ADDRESS'), 'PHONE_NUMBER', 'CUST_EMAIL', 'ACCOUNT_MGR_ID', 'DATE_OF_BIRTH', 'MARITAL_STATUS', 'GENDER') \n",
    "mocked_df.coalesce(1).write.mode('append').format('csv').option(\"header\", True).save(\"s3://de-40-training-raw/final_exam_data/customers/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14ba89d1-f71a-4a41-b840-9b60d688a868",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Generate test data for products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3654611-cdae-484c-a1b9-f6d4e7cb326a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "product_root_path = \"s3://de-40-training-raw/final_exam_data/products/\"\n",
    "df = spark.read.csv(product_root_path + \"products.csv\", header=True) \n",
    "\n",
    "category_list = [\n",
    "   \"hardware1\"\n",
    "  ,\"hardware3\"\n",
    "  ,\"hardware4\"\n",
    "  ,\"hardware2\"\n",
    "]\n",
    "product_status_list = [\n",
    "   \"obsolete\"\n",
    "  ,\"planned\"\n",
    "  ,\"under development\"\n",
    "  ,\"orderable\"\n",
    "]\n",
    "\n",
    "\n",
    "@F.udf(StringType())\n",
    "def fake_price(): return  faker.random_int(min=1, max=999)\n",
    "\n",
    "@F.udf(returnType=StringType())\n",
    "def rand_category(): return random.choice(category_list)\n",
    "@F.udf(returnType=StringType())\n",
    "def rand_product_status(): return random.choice(product_status_list)\n",
    "\n",
    "sampled_df = sample_n_rows(df, 10)  # Take 10 random rows \n",
    " \n",
    "mocked_df = sampled_df.select('PRODUCT_ID', 'PRODUCT_NAME',  rand_category().alias('CATEGORY_NAME'), 'WEIGHT_CLASS', rand_product_status().alias('PRODUCT_STATUS'), fake_price().alias('LIST_PRICE'), fake_price().alias('MIN_PRICE'))\n",
    "mocked_df.coalesce(1).write.mode('append').format('csv').option(\"header\", True).save(product_root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94850861-0c25-47c5-8b0c-27f8256a0aec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Generate test data for orders & oreder items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f2e41f6-a853-4965-b11b-8da373c3a384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "number_of_orders = 10\n",
    "\n",
    " \n",
    "def fake_formatted_date():\n",
    "    random_date = faker.date_between(start_date='-3y', end_date='today')\n",
    "    return str(random_date.strftime(\"%d-%b-%y\").upper())  # e.g., '16-AUG-07'\n",
    "\n",
    "# Register for SQL use\n",
    "spark.udf.register(\"fake_formatted_date\", fake_formatted_date)\n",
    "\n",
    "\n",
    "order_modes = [\"direct\",\"online\"]\n",
    "order_root = \"s3://de-40-training-raw/final_exam_data/orders/\"\n",
    "order_item_root = \"s3://de-40-training-raw/final_exam_data/order_items/\"\n",
    "max_order = int(spark.read.format('csv').option(\"header\", True).load(order_root).select(\"order_id\").agg(F.max(\"order_id\")).collect()[0][0])\n",
    "product_id_list = [3057,1791,2415,3399,3072,2252,3060,3354,3054,2253,3350,1782,2359,1792,1772,3355,2459,3353,3061,2453,3069,2243,2257,2410,2245,2395,3334,2254,2236,1755,3065,2302,1768,3234,3127,3071,3064,3400,3331,1726,2255,1743,1797,2430,2404,3073,2406,3155,2414,2382]\n",
    "\n",
    "customer_id_list = [169,272,155,154,101,162,146,160,184,171,183,276,179,267,174,274,166,268,147,164,177,102,263,167,270,271,156,144,176,150,170,153,148,180,159,158,266,152,149,165,275,172,181,265,157,175,161,151,262,145]\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "orders_df = spark.read.format('csv').option(\"header\", True).load(order_root+\"orders.csv\")\n",
    "orders_df_schema = orders_df.schema \n",
    " \n",
    "\n",
    "order_items_df = spark.read.format('csv').option(\"header\", True).load(order_item_root+\"order_items.csv\")\n",
    "order_items_df_schema = order_items_df.schema \n",
    "\n",
    "res_orders = []\n",
    "res_order_items = []\n",
    "for i in range(0, number_of_orders):\n",
    "    order_id = int(max_order) + i\n",
    "    order_date = str(fake_formatted_date())\n",
    "    order_mode = random.choice(order_modes)\n",
    "    customer_id = random.choice(customer_id_list)\n",
    "    ORDER_STATUS = 0\n",
    "    order_total = faker.pyfloat(left_digits=3, right_digits=2, positive=True)\n",
    "    sales_rep_id = -1 \n",
    "    promotion_id = '-' \n",
    "    LINE_ITEM_ID = 1\n",
    "    PRODUCT_ID =  random.choice(product_id_list)\n",
    "    UNIT_PRICE = faker.pyfloat(left_digits=2, right_digits=2, positive=True)\n",
    "    QUANTITY = faker.random_int(min=1, max=9)\n",
    "\n",
    "    temp_tuple_order = (order_id, order_date,order_mode, customer_id, ORDER_STATUS, order_total, sales_rep_id, promotion_id)\n",
    "    temp_tuple_order_item = (order_id,LINE_ITEM_ID,PRODUCT_ID,UNIT_PRICE,QUANTITY)\n",
    "    res_orders.append(temp_tuple_order)\n",
    "    res_order_items.append(temp_tuple_order_item)\n",
    "print(res_orders)\n",
    "mocked_orders_df = spark.createDataFrame(res_orders, schema=orders_df.schema)\n",
    "mocked_orders_df.display()\n",
    "\n",
    "\n",
    "mocked_order_items_df = spark.createDataFrame(res_order_items, schema=order_items_df_schema)\n",
    "mocked_order_items_df.display()\n",
    "mocked_orders_df.coalesce(1).write.format('csv').mode(\"append\").option(\"header\", True).save(order_root)\n",
    "mocked_order_items_df.coalesce(1).write.format('csv').mode(\"append\").option(\"header\", True).save(order_item_root)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb8b2948-54f7-4336-b030-8be652e59454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(order_items_df_schema)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Exam generate test data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}