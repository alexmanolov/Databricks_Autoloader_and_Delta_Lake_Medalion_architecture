{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74d7968e-a3b9-4f1c-82d6-85aae9f564cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Task 2\n",
    "Overview: Standardize and load the bronze data into silver layer.\n",
    "- Perform following transformations where applicable:\n",
    "o All fields to be named in snake_case, replace all dots with\n",
    "underscores.\n",
    "o Phone numbers to be separated by dashes (f.e 123-321-567)\n",
    "o All date and datetime fields to be loaded with the proper data type\n",
    "(date & timestamp)\n",
    "- For target schema use your buddy group schema\n",
    "- Name the silver tables: <name>_silver__<orders/customers/etc.>. For\n",
    "example: kirilovl_silver_orders\n",
    "- In case of conflicts with already existing tables in your schema, add _exam\n",
    "suffix\n",
    "- Keep all rows, but no historization is required\n",
    "- Include bronze ingest_datetime and rename it consume_datetime\n",
    "- Exclude batch_id from source\n",
    "- Add inserted_datetime to track the time when the row was loaded.\n",
    "- Add deduplication to exclude duplicate rows on all business columns, keep\n",
    "the first consumed record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb2ba3e5-f2c9-404e-8d16-8d5110aaf2c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###################\n",
    "##### Silver ######\n",
    "###################\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace, col, concat, lit, substring, length, current_timestamp, to_date, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "###################\n",
    "# Customers table #\n",
    "###################\n",
    "\n",
    "# Load bronze customers table\n",
    "bronze_customers_df = spark.table(\"de_pyspark_training_catalog.buddy_group_1.amanolov_bronze_customers_exam\")\n",
    "\n",
    "# Rename columns to snake_case and replace . with _\n",
    "for col_name in bronze_customers_df.columns:\n",
    "    new_col_name = col_name.lower().replace('.', '_')\n",
    "    bronze_customers_df = bronze_customers_df.withColumnRenamed(col_name, new_col_name)\n",
    "\n",
    "# Rename ingest_datetime to consume_datetime\n",
    "silver_customers_df = bronze_customers_df.withColumnRenamed(\"ingest_datetime\", \"consume_datetime\")\n",
    "\n",
    "# Drop batch_id\n",
    "silver_customers_df = silver_customers_df.drop(\"batch_id\")\n",
    "\n",
    "# phone_number conversion with regexp_replace\n",
    "# 1. Remove all non-digit characters except +\n",
    "silver_customers_df = silver_customers_df.withColumn(\n",
    "    \"phone_number\",\n",
    "    regexp_replace(col(\"phone_number\"), \"[^\\\\d+]\", \"\")\n",
    ")\n",
    "\n",
    "# 2. Extract country code (assume always '+1')\n",
    "silver_customers_df = silver_customers_df.withColumn(\n",
    "    \"country_code\",\n",
    "    substring(col(\"phone_number\"), 1, 2)\n",
    ")\n",
    "\n",
    "# 3. Extract rest of the phone No\n",
    "silver_customers_df = silver_customers_df.withColumn(\n",
    "    \"clean_phone_number\",\n",
    "    substring(col(\"phone_number\"), 3, 10)\n",
    ")\n",
    "\n",
    "# 4. Format as XXX-XXX-XXXX\n",
    "silver_customers_df = silver_customers_df.withColumn(\n",
    "    \"formatted_phone\",\n",
    "    regexp_replace(col(\"clean_phone_number\"), \"(\\\\d{3})(\\\\d{3})(\\\\d{4})\", \"$1-$2-$3\")\n",
    ")\n",
    "\n",
    "# 5. Combine country code and formatted number\n",
    "silver_customers_df = silver_customers_df.withColumn(\n",
    "    \"phone_number\",\n",
    "    concat(col(\"country_code\"), lit(\" \"), col(\"formatted_phone\"))\n",
    ")\n",
    "\n",
    "# 6. Drop temporary cols\n",
    "silver_customers_df = silver_customers_df.drop(\"country_code\", \"clean_phone_number\", \"formatted_phone\")\n",
    "\n",
    "\n",
    "# Cast date_of_birth to date type\n",
    "silver_customers_df = silver_customers_df.withColumn(\"date_of_birth\", to_date(col(\"date_of_birth\")))\n",
    "\n",
    "# Add inserted_datetime\n",
    "silver_customers_df = silver_customers_df.withColumn(\"inserted_datetime\", current_timestamp())\n",
    "\n",
    "# Deduplicate on customer_id, keep earliest consume_datetime\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(\"consume_datetime\")\n",
    "silver_customers_df = silver_customers_df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "silver_customers_df = silver_customers_df.filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "\n",
    "# Save to silver table\n",
    "(silver_customers_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"de_pyspark_training_catalog.buddy_group_1.amanolov_silver_customers_exam\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47c6e7de-2e14-4ea1-8149-a26dc81a8a1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+---------+-------+\n",
      "|col_name                   |data_type|comment|\n",
      "+---------------------------+---------+-------+\n",
      "|CUSTOMER_ID                |string   |NULL   |\n",
      "|CUST_FIRST_NAME            |string   |NULL   |\n",
      "|CUST_LAST_NAME             |string   |NULL   |\n",
      "|CUST_ADDRESS.COUNTRY_ID    |string   |NULL   |\n",
      "|CUST_ADDRESS.STATE_PROVINCE|string   |NULL   |\n",
      "|CUST_ADDRESS.CITY          |string   |NULL   |\n",
      "|CUST_ADDRESS.POSTAL_CODE   |string   |NULL   |\n",
      "|CUST_ADDRESS.STREET_ADDRESS|string   |NULL   |\n",
      "|PHONE_NUMBER               |string   |NULL   |\n",
      "|CUST_EMAIL                 |string   |NULL   |\n",
      "|ACCOUNT_MGR_ID             |string   |NULL   |\n",
      "|DATE_OF_BIRTH              |string   |NULL   |\n",
      "|MARITAL_STATUS             |string   |NULL   |\n",
      "|GENDER                     |string   |NULL   |\n",
      "|_rescued_data              |string   |NULL   |\n",
      "|ingest_datetime            |timestamp|NULL   |\n",
      "|batch_id                   |int      |NULL   |\n",
      "+---------------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look for column names to replace above\n",
    "spark.sql(\"\"\"\n",
    "    DESCRIBE de_pyspark_training_catalog.buddy_group_1.amanolov_bronze_customers_exam\n",
    "\"\"\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8781822f-174a-4613-96c2-1175de125e74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|phone_number   |\n",
      "+---------------+\n",
      "|+1 317-123-4104|\n",
      "|+1 317-123-4111|\n",
      "|+1 319-123-4301|\n",
      "|+1 745-123-4306|\n",
      "|+1 414-123-4307|\n",
      "|+1 414-123-4308|\n",
      "|+1 608-123-4309|\n",
      "|+1 608-123-4318|\n",
      "|+1 414-123-4323|\n",
      "|+1 414-123-4324|\n",
      "|+1 414-123-4328|\n",
      "|+1 608-123-4332|\n",
      "|+1 608-123-4344|\n",
      "|+1 414-123-4347|\n",
      "|+1 414-123-4348|\n",
      "|+1 414-123-4350|\n",
      "|+1 745-123-4367|\n",
      "|+1 414-123-4369|\n",
      "|+1 715-123-4372|\n",
      "|+1 414-123-4373|\n",
      "+---------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Check if phone_number conversion is correct\n",
    "spark.sql(\"\"\"\n",
    "    SELECT phone_number FROM de_pyspark_training_catalog.buddy_group_1.amanolov_silver_customers_exam\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7164f2e-087e-433d-b2d8-91a47cae0ae2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###################\n",
    "##### Silver ######\n",
    "###################\n",
    "\n",
    "###################\n",
    "# Orders table #\n",
    "###################\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Load Bronze Orders\n",
    "bronze_orders_df = spark.table(\"de_pyspark_training_catalog.buddy_group_1.amanolov_bronze_orders_exam\")\n",
    "\n",
    "# Rename Columns to snake_case and replace . with _\n",
    "for col_name in bronze_orders_df.columns:\n",
    "    new_col_name = col_name.lower().replace('.', '_')\n",
    "    bronze_orders_df = bronze_orders_df.withColumnRenamed(col_name, new_col_name)\n",
    "\n",
    "# Rename ingest_datetime to consume_datetime\n",
    "silver_orders_df = bronze_orders_df.withColumnRenamed(\"ingest_datetime\", \"consume_datetime\")\n",
    "\n",
    "# Drop batch_id\n",
    "silver_orders_df = silver_orders_df.drop(\"batch_id\")\n",
    "\n",
    "# Register temp view for SQL workaround\n",
    "silver_orders_df.createOrReplaceTempView(\"temp_silver_orders\")\n",
    "\n",
    "# Parse the order_date using SQL workaround\n",
    "silver_orders_df = spark.sql(\"\"\"\n",
    "    SELECT *,\n",
    "        coalesce(\n",
    "            try_to_timestamp(regexp_replace(order_date, '\\\\\\\\.', ':'), 'dd-MMM-yy hh:mm:ss a'),\n",
    "            try_to_timestamp(regexp_replace(order_date, '\\\\\\\\.', ':'), 'dd-MMM-yy')\n",
    "        ) AS clean_order_date\n",
    "    FROM temp_silver_orders\n",
    "\"\"\")\n",
    "\n",
    "# Replace the old order_date with the cleaned one\n",
    "silver_orders_df = silver_orders_df.drop(\"order_date\").withColumnRenamed(\"clean_order_date\", \"order_date\")\n",
    "\n",
    "# Add inserted_datetime\n",
    "silver_orders_df = silver_orders_df.withColumn(\"inserted_datetime\", current_timestamp())\n",
    "\n",
    "# Deduplicate on order_id\n",
    "window_spec = Window.partitionBy(\"order_id\").orderBy(\"consume_datetime\")\n",
    "silver_orders_df = silver_orders_df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "silver_orders_df = silver_orders_df.filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "\n",
    "# Save with schema overwrite\n",
    "(silver_orders_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(\"de_pyspark_training_catalog.buddy_group_1.amanolov_silver_orders_exam\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9e849b8-215c-4533-8a7d-905211c5de33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------+\n",
      "|col_name       |data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|ORDER_ID       |string   |NULL   |\n",
      "|ORDER_DATE     |string   |NULL   |\n",
      "|ORDER_MODE     |string   |NULL   |\n",
      "|CUSTOMER_ID    |string   |NULL   |\n",
      "|ORDER_STATUS   |string   |NULL   |\n",
      "|ORDER_TOTAL    |string   |NULL   |\n",
      "|SALES_REP_ID   |string   |NULL   |\n",
      "|PROMOTION_ID   |string   |NULL   |\n",
      "|_rescued_data  |string   |NULL   |\n",
      "|ingest_datetime|timestamp|NULL   |\n",
      "|batch_id       |int      |NULL   |\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look for column names to replace above\n",
    "spark.sql(\"\"\"\n",
    "    DESCRIBE de_pyspark_training_catalog.buddy_group_1.amanolov_bronze_orders_exam\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c40ee2ed-bd62-4025-82e1-b27ea660418c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###################\n",
    "##### Silver ######\n",
    "###################\n",
    "\n",
    "###################\n",
    "# Order_items table #\n",
    "###################\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Load bronze order_items table\n",
    "bronze_order_items_df = spark.table(\"de_pyspark_training_catalog.buddy_group_1.amanolov_bronze_order_items_exam\")\n",
    "\n",
    "# Rename columns to snake_case and replace . with _\n",
    "for col_name in bronze_order_items_df.columns:\n",
    "    new_col_name = col_name.lower().replace('.', '_')\n",
    "    bronze_order_items_df = bronze_order_items_df.withColumnRenamed(col_name, new_col_name)\n",
    "\n",
    "# Rename ingest_datetime to consume_datetime\n",
    "silver_order_items_df = bronze_order_items_df.withColumnRenamed(\"ingest_datetime\", \"consume_datetime\")\n",
    "\n",
    "# Drop batch_id\n",
    "silver_order_items_df = silver_order_items_df.drop(\"batch_id\")\n",
    "\n",
    "# Add inserted_datetime\n",
    "silver_order_items_df = silver_order_items_df.withColumn(\"inserted_datetime\", current_timestamp())\n",
    "\n",
    "# Deduplicate on order_id and line_item_id, keep earliest consume_datetime\n",
    "window_spec = Window.partitionBy(\"order_id\", \"line_item_id\").orderBy(\"consume_datetime\")\n",
    "silver_order_items_df = silver_order_items_df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "silver_order_items_df = silver_order_items_df.filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "\n",
    "# Save to silver table\n",
    "(silver_order_items_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"de_pyspark_training_catalog.buddy_group_1.amanolov_silver_order_items_exam\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "542575ab-f893-4883-a4c0-24d255372a5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------+\n",
      "|col_name       |data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|ORDER_ID       |string   |NULL   |\n",
      "|LINE_ITEM_ID   |string   |NULL   |\n",
      "|PRODUCT_ID     |string   |NULL   |\n",
      "|UNIT_PRICE     |string   |NULL   |\n",
      "|QUANTITY       |string   |NULL   |\n",
      "|_rescued_data  |string   |NULL   |\n",
      "|ingest_datetime|timestamp|NULL   |\n",
      "|batch_id       |int      |NULL   |\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    DESCRIBE de_pyspark_training_catalog.buddy_group_1.amanolov_bronze_order_items_exam\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f797341d-1cb3-4cac-9d8f-fc7cb014b90c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###################\n",
    "##### Silver ######\n",
    "###################\n",
    "\n",
    "###################\n",
    "# products table #\n",
    "###################\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Load bronze products table\n",
    "bronze_products_df = spark.table(\"de_pyspark_training_catalog.buddy_group_1.amanolov_bronze_products_exam\")\n",
    "\n",
    "# Rename columns to snake_case and replace dots with underscores\n",
    "for col_name in bronze_products_df.columns:\n",
    "    new_col_name = col_name.lower().replace('.', '_')\n",
    "    bronze_products_df = bronze_products_df.withColumnRenamed(col_name, new_col_name)\n",
    "\n",
    "# Rename ingest_datetime to consume_datetime\n",
    "silver_products_df = bronze_products_df.withColumnRenamed(\"ingest_datetime\", \"consume_datetime\")\n",
    "\n",
    "# Drop batch_id\n",
    "silver_products_df = silver_products_df.drop(\"batch_id\")\n",
    "\n",
    "# Add inserted_datetime\n",
    "silver_products_df = silver_products_df.withColumn(\"inserted_datetime\", current_timestamp())\n",
    "\n",
    "# Deduplicate on product_id, keep earliest consume_datetime\n",
    "window_spec = Window.partitionBy(\"product_id\").orderBy(\"consume_datetime\")\n",
    "silver_products_df = silver_products_df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "silver_products_df = silver_products_df.filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "\n",
    "# Save to silver table\n",
    "(silver_products_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"de_pyspark_training_catalog.buddy_group_1.amanolov_silver_products_exam\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d28e88b3-823d-4ec1-88af-7f011efa01a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------+\n",
      "|col_name       |data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|PRODUCT_ID     |string   |NULL   |\n",
      "|PRODUCT_NAME   |string   |NULL   |\n",
      "|CATEGORY_NAME  |string   |NULL   |\n",
      "|WEIGHT_CLASS   |string   |NULL   |\n",
      "|PRODUCT_STATUS |string   |NULL   |\n",
      "|LIST_PRICE     |string   |NULL   |\n",
      "|MIN_PRICE      |string   |NULL   |\n",
      "|_rescued_data  |string   |NULL   |\n",
      "|ingest_datetime|timestamp|NULL   |\n",
      "|batch_id       |int      |NULL   |\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    DESCRIBE de_pyspark_training_catalog.buddy_group_1.amanolov_bronze_products_exam\n",
    "\"\"\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5375633204748151,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Final exam",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
